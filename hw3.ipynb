{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1f36297f19619f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## I modified from my own homework for Prof. Tran's AE598RL course last term. The original code is here: https://github.com/uiuc-ae598-rl-2023-spring/hw1-dp-LXYYY.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:27:17.369104Z",
     "start_time": "2024-04-04T06:27:17.323789Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeee1c7d930a34",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "## (a)\n",
    "\n",
    "Shown by the dynamic programming, the optimal policy is rather obvious in this question. Coin A is a `certain` coin, and so the policy is to always choose A when not reaching the 8 heads, and choose B afterward. In other words, the game is only dependent on the two B coin flips. And the win rate is 0.5^2=0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbeb9a79b4db532a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:26:40.912878Z",
     "start_time": "2024-04-04T07:26:40.908802Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy sequence: ['B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n",
      "Simulated win probability: 0.25\n"
     ]
    }
   ],
   "source": [
    "def coin_game_DP():\n",
    "    # Initialize the DP table; +1 for extra head counts and +1 for 0-based indexing\n",
    "    dp = [[0 for _ in range(12)] for _ in\n",
    "          range(11)]  # 11 for the number of flips (0-10), 12 for heads (0-11, with 9+ as loss)\n",
    "    policy = [['' for _ in range(12)] for _ in range(11)]\n",
    "\n",
    "    # Base cases\n",
    "    dp[10][8] = 1  # Win condition if exactly 8 heads after 10 flips\n",
    "    for h in range(9, 12):  # Lose condition if more than 8 heads\n",
    "        dp[10][h] = 0\n",
    "\n",
    "    # DP table fill\n",
    "    for n in range(9, -1, -1):  # From 9 down to 0 flips\n",
    "        for h in range(8, -1, -1):  # Up to 8 heads, inclusive\n",
    "            # Coin A choice leads directly to the next state with one more head\n",
    "            probA = dp[n + 1][min(h + 1, 11)]  # min to cap heads at 11 (9+ considered as losing states)\n",
    "\n",
    "            # Coin B choice, with a fair chance of head or tail\n",
    "            probB = 0.5 * dp[n + 1][min(h + 1, 11)] + 0.5 * dp[n + 1][h]\n",
    "\n",
    "            # Select the action with the higher expected probability\n",
    "            if probA > probB:\n",
    "                dp[n][h] = probA\n",
    "                policy[n][h] = 'A'  # Choose coin A\n",
    "            else:\n",
    "                dp[n][h] = probB\n",
    "                policy[n][h] = 'B'  # Choose coin B\n",
    "\n",
    "    # Reconstruct the policy path\n",
    "    n, h = 0, 0\n",
    "    path = []\n",
    "    while n < 10:\n",
    "        decision = policy[n][h]\n",
    "        path.append(decision)\n",
    "        if decision == 'A':\n",
    "            h = min(h + 1, 11)  # Increment head count or cap\n",
    "        n += 1\n",
    "\n",
    "    return dp[0][0], path\n",
    "\n",
    "\n",
    "def simulate_policy(policy):\n",
    "    probability = 1.0  # Start with 100% probability\n",
    "    heads = 0  # Initial number of heads\n",
    "\n",
    "    # Simulate each decision in the policy\n",
    "    for n in range(10):  # For each flip\n",
    "        decision = policy[n]\n",
    "        if decision == 'A':\n",
    "            # Coin A (guaranteed head)\n",
    "            heads += 1\n",
    "            # Probability does not change as outcome is certain\n",
    "        elif decision == 'B':\n",
    "            # Coin B (fair coin, 50% head)\n",
    "            if heads < 8:\n",
    "                # Only if less than 8 heads, flipping coin B makes sense for trying to win\n",
    "                probability *= 0.5  # Update probability for the uncertain outcome\n",
    "\n",
    "        # If at any point heads exceed 8, the game is lost, so probability is 0\n",
    "        if heads > 8:\n",
    "            return 0.0\n",
    "\n",
    "    # If exactly 8 heads, return the accumulated probability, else 0\n",
    "    return probability\n",
    "\n",
    "\n",
    "_, policy_path = coin_game_DP()\n",
    "print(f\"Policy sequence: {policy_path}\")\n",
    "print(f\"Simulated win probability: {simulate_policy(policy_path)}\")  # Should match the DP result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709355dcb814405",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "## My evaluation on the three policies:\n",
    "\n",
    "### (1) and (3)\n",
    "By my understanding, the first and third policy is not valid in this problem setup. Because if Q- and V- tables are trained in a fully observable MDP, the `locate` action then will make no benefit for the agent. And when we use the policy as the given styles, the agent will always intend to locate itself in the good state, with no knowledge it won't yield any reward.\n",
    "\n",
    "### (2)\n",
    "The second policy is a policy that will gives some rewards, because it always chooses between the actions rewarded in full observable MDP.\n",
    "\n",
    "and my evaluation shows the policy 1 and 3 give -1 average rewards, but random policy gives a very small but positive reward around 0.0004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684eacac2784229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:57:20.495781Z",
     "start_time": "2024-04-04T07:57:20.482162Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from good_bad import GoodBad\n",
    "\n",
    "env = GoodBad()\n",
    "\n",
    "# from models.policy_iteration.policy_iteration import learn\n",
    "from models.value_iteration.value_iteration import learn\n",
    "\n",
    "# from models.q_learning.q_learning import learn\n",
    "model = learn(env, scene=\"good_bad\", max_it=1000, gamma=0.95, epsilon=0.3, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5fdde6f269c1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:57:32.465127Z",
     "start_time": "2024-04-04T07:57:22.210882Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward with policy (1): -1.009999\n",
      "Total reward with policy (2): 0.000385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "s = env.reset()\n",
    "\n",
    "max_sim_steps = 10000\n",
    "\n",
    "from models.base_model import ModelBasedAlg\n",
    "\n",
    "\n",
    "def get_action_from_belief(model: ModelBasedAlg, b, s, random):\n",
    "    v0 = model.get_values(0)\n",
    "    v1 = model.get_values(1)\n",
    "    if random:\n",
    "        model_policy_0 = model.get_policy(0)\n",
    "        model_policy_1 = model.get_policy(1)\n",
    "        return np.random.choice([model_policy_0, model_policy_1], p=b)\n",
    "    else:\n",
    "        # try all action\n",
    "        max_value = -np.inf\n",
    "        best_action = None\n",
    "        for a in [0, 1, 2]:\n",
    "            new_b = model.update_belief(s, b, a)\n",
    "            new_value = new_b[0] * v0 + new_b[1] * v1\n",
    "            if new_value > max_value:\n",
    "                max_value = new_value\n",
    "                best_action = a\n",
    "        return best_action\n",
    "\n",
    "env.max_num_steps=max_sim_steps\n",
    "max_sims=100000\n",
    "\n",
    "b = np.asarray([1, 0])\n",
    "r_record=[]\n",
    "for experiment in range(max_sims*10):\n",
    "    r_sum = 0\n",
    "    for i in range(max_sim_steps):\n",
    "        a = get_action_from_belief(model, b, s, random=False)\n",
    "        b = model.update_belief(s, b, a)\n",
    "        s1, r, done = env.step(a)\n",
    "        r_sum += r\n",
    "        s = s1\n",
    "        if done:\n",
    "            r_record.append(r_sum)\n",
    "            break\n",
    "print(f\"Average reward with policy (1): {np.mean(r_record)}\")\n",
    "\n",
    "s = env.reset()\n",
    "r_sum = 0\n",
    "b = np.asarray([1, 0])\n",
    "r_record=[]\n",
    "for experiment in range(max_sims*10):\n",
    "    r_sum = 0\n",
    "    for i in range(max_sim_steps):\n",
    "        a = get_action_from_belief(model, b, s, random=True)\n",
    "        b = model.update_belief(s, b, a)\n",
    "        s1, r, done = env.step(a)\n",
    "        r_sum += r\n",
    "        s = s1\n",
    "        if done:\n",
    "            r_record.append(r_sum)\n",
    "            break\n",
    "print(f\"Total reward with policy (2): {np.mean(r_record)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63da4d27a678ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:57:34.425370Z",
     "start_time": "2024-04-04T07:57:34.072791Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 of 1000 finished\n",
      "Episode 100 of 1000 finished\n",
      "Episode 200 of 1000 finished\n",
      "Episode 300 of 1000 finished\n",
      "Episode 400 of 1000 finished\n",
      "Episode 500 of 1000 finished\n",
      "Episode 600 of 1000 finished\n",
      "Episode 700 of 1000 finished\n",
      "Episode 800 of 1000 finished\n",
      "Episode 900 of 1000 finished\n"
     ]
    }
   ],
   "source": [
    "from models.q_learning.q_learning import learn as q_learn\n",
    "from models.q_learning.q_learning import PRECISE\n",
    "from good_bad import GoodBad\n",
    "\n",
    "env = GoodBad(100)\n",
    "model = q_learn(env, scene=\"good_bad\", max_it=1000, gamma=0.9, epsilon=0.3, alpha=0.1, obs_mode=PRECISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fa20f975fb9aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:57:45.180441Z",
     "start_time": "2024-04-04T07:57:42.446232Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward with policy (1): -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from models.base_model import ModelFreeAlg\n",
    "\n",
    "def get_action_from_belief(model: ModelFreeAlg, b ):\n",
    "    Q0= model.get_Q(0)\n",
    "    Q1= model.get_Q(1)\n",
    "    b_Q0=b[0]*Q0\n",
    "    b_Q1=b[1]*Q1\n",
    "    b_Q_sum=b_Q0+b_Q1\n",
    "    # find the action with the highest value\n",
    "    best_action=None\n",
    "    highest_value=-np.inf\n",
    "    for a in [0, 1, 2]:\n",
    "        if b_Q_sum[a]>highest_value:\n",
    "            highest_value=b_Q_sum[a]\n",
    "            best_action=a\n",
    "    return best_action\n",
    "        \n",
    "env.max_num_steps=10000\n",
    "    \n",
    "s = env.reset()\n",
    "b = np.asarray([1, 0])\n",
    "r_record=[]\n",
    "for experiment in range(max_sims*10):\n",
    "    r_sum = 0\n",
    "    for i in range(max_sim_steps):\n",
    "        a = get_action_from_belief(model, b)\n",
    "        b=model.update_belief(s, b, a)\n",
    "        s1, r, done = env.step(a)\n",
    "        r_sum += r\n",
    "        s = s1\n",
    "        if done:\n",
    "            r_record.append(r_sum)\n",
    "            break\n",
    "print(f\"Total reward with policy (1): {r_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24d0aca4b8f71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Problem 3:\n",
    "\n",
    "## (a)\n",
    "\n",
    "Firstly I defined this `MountainGridWorld` class to simulate the game. I chose an implementation which I believe is close to real world setup. I defined the step output of the game as `groundtruth state, reward, done, noisey state observation, and confidence`, where the ground truth state is only for bookkeeping, not used during training. The confidence is computed from the altitude of the ground truth state.\n",
    "\n",
    "The reward is defined as such: if the agent reaches the goal, the reward is 100, and game is `done`. If an unfeasible action is taken, the reward is -10, and cause the game to be `done`. Other intermediate steps are time-penalised by -1. For simplicity, the agent itself doesn't know which action direction is not feasible, it has to learn from the environment.\n",
    "\n",
    "Then I discretize the confidence as a resolution of 0.1 and then combined the observed state and confidence into a single state representation, which has `49*10=490` states.\n",
    "\n",
    "Then I trained the policy with Q-learning approach, with the state space as aforementioned, 490 states, and 4 actions, where the reward of each step follows the method given in the lecture, where the rewards of belief is computed based on the distribution of the uncertainty. A epsilon-exploration approach is also applied, i.e. during training, there is 0.3 of chance to take a random action. \n",
    "\n",
    "For inference, I simply take the action with the highest value in the Q-table, where the state is combined observation-confidence state.\n",
    "\n",
    "## (b)\n",
    "\n",
    "My opinion on the optimality of my approach, is that the usage of belief-aware Q-table and reward distribution is a good approach. And there is no \"locate\" action in this simple game, I believe my approach is sufficient. However, my implementation can be improved by the following ways:\n",
    "1. One can consider to use a continuous state space, instead of discretization of the belief. But given the uncertainty is inherently discrete, and it's not accumulating because it's GPS-like, I believe the discretization is a good choice.\n",
    "2. the belief can have more dimension, e.g. a 2d distribution instead of a single-value confidence. But in this game, the uncertainty is isotropic, so a single value should be enough.\n",
    "3. given the fact the uncertainty of the game is between a few discrete values, the discretization can be more tailored, which should be a valid improvement\n",
    "\n",
    "## (c)\n",
    "\n",
    "I evaluated my model by running 10000 experiments, and the average number of steps is: ``14.227``. Meanwhile, I found the training is a bit sensitive to the hyperparameters, e.g. with lower discount factor and alpha, I got around 25 steps, and around 1% fail rate, and the best experiment, once I got around 9 steps average, which however, failed to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e67ed26051a21ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:00:22.861230Z",
     "start_time": "2024-04-04T07:00:09.960693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 of 100000 finished\n",
      "Episode 10000 of 100000 finished\n",
      "Episode 20000 of 100000 finished\n",
      "Episode 30000 of 100000 finished\n",
      "Episode 40000 of 100000 finished\n",
      "Episode 50000 of 100000 finished\n",
      "Episode 60000 of 100000 finished\n",
      "Episode 70000 of 100000 finished\n",
      "Episode 80000 of 100000 finished\n",
      "Episode 90000 of 100000 finished\n"
     ]
    }
   ],
   "source": [
    "from mountain_gridworld import MountainGridWorld\n",
    "\n",
    "env = MountainGridWorld()\n",
    "\n",
    "from models.q_learning.q_learning import learn as q_learn\n",
    "from models.q_learning.q_learning import PRECISE, NOISY\n",
    "\n",
    "model = q_learn(env, scene=\"mountain_gridworld\", max_it=100000, gamma=0.95, epsilon=0.3, alpha=0.2, obs_mode=NOISY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30830f317a7bdd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T07:00:37.852544Z",
     "start_time": "2024-04-04T07:00:37.498693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for experiment 0: 92\n",
      "Total reward for experiment 500: 94\n",
      "Total reward for experiment 1000: 90\n",
      "Total reward for experiment 1500: 94\n",
      "Total reward for experiment 2000: 90\n",
      "Total reward for experiment 2500: 90\n",
      "Total reward for experiment 3000: 90\n",
      "Total reward for experiment 3500: 88\n",
      "Total reward for experiment 4000: 82\n",
      "Total reward for experiment 4500: 88\n",
      "Total reward for experiment 5000: 84\n",
      "Total reward for experiment 5500: 80\n",
      "Total reward for experiment 6000: 92\n",
      "Total reward for experiment 6500: 86\n",
      "Total reward for experiment 7000: 86\n",
      "Total reward for experiment 7500: 76\n",
      "Total reward for experiment 8000: 94\n",
      "Total reward for experiment 8500: 94\n",
      "Total reward for experiment 9000: 86\n",
      "Total reward for experiment 9500: 94\n",
      "Average time cost: 14.227\n",
      "Average time cost of successful experiments: 14.227\n",
      "Number of fails: 0\n"
     ]
    }
   ],
   "source": [
    "from models.q_learning.q_learning import discretize_state_and_confidence, DISCRETIZATION_RESOLUTION\n",
    "import numpy as np\n",
    "\n",
    "time_cost=[]\n",
    "num_fails=0\n",
    "for experiment in range(10000):\n",
    "    # test\n",
    "    s = env.reset()\n",
    "    obs = s\n",
    "    confidence = 1\n",
    "    r_sum = 0\n",
    "    num_step=0\n",
    "    for i in range(100):\n",
    "        s_obs_and_confidence = discretize_state_and_confidence(obs, confidence)\n",
    "        a = model.get_policy(s_obs_and_confidence)\n",
    "        s1, r, done, obs, confidence = env.step(a)\n",
    "        r_sum += r\n",
    "        s = s1\n",
    "        num_step+=1\n",
    "        if done or i==99:\n",
    "            if r_sum>0: # if the agent reaches the goal\n",
    "                time_cost.append(num_step)\n",
    "            else:\n",
    "                num_fails+=1\n",
    "                time_cost.append(100)\n",
    "                break\n",
    "            break\n",
    "    if experiment%500==0:\n",
    "        print(f\"Total reward for experiment {experiment}: {r_sum}\")\n",
    "print(f\"Average time cost: {np.mean(time_cost)}\")\n",
    "print(f\"Average time cost of successful experiments: {np.mean([t for t in time_cost if t<99])}\")\n",
    "print(f\"Number of fails: {num_fails}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
